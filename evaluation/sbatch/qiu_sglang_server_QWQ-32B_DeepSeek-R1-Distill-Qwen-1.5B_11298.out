Mon Aug 25 16:33:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          Off |   00000000:4F:00.0 Off |                    0 |
| N/A   50C    P0            293W /  300W |   66763MiB /  81920MiB |    100%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          Off |   00000000:52:00.0 Off |                    0 |
| N/A   27C    P0             43W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          Off |   00000000:56:00.0 Off |                    0 |
| N/A   27C    P0             63W /  300W |   43627MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          Off |   00000000:57:00.0 Off |                    0 |
| N/A   27C    P0             43W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          Off |   00000000:D5:00.0 Off |                    0 |
| N/A   27C    P0             42W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   2223691      C   python                                      66754MiB |
|    2   N/A  N/A   2258570      C   /home/shenhui/miniconda3/bin/python         43618MiB |
+-----------------------------------------------------------------------------------------+
/home/xiongjing
Waiting for server on port 40000 to start...
INFO 08-25 16:33:37 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 16:33:38 __init__.py:190] Automatically detected platform cuda.
Waiting for server on port 40000 to start...
[2025-08-25 16:33:40] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=40001, mem_fraction_static=0.9, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=1, stream_interval=1, stream_output=False, random_seed=510706285, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)
[2025-08-25 16:33:40] server_args=ServerArgs(model_path='Qwen/QwQ-32B', tokenizer_path='Qwen/QwQ-32B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/QwQ-32B', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=40000, mem_fraction_static=0.9, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=1, stream_interval=1, stream_output=False, random_seed=26836944, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)
Waiting for server on port 40000 to start...
INFO 08-25 16:33:53 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 16:33:54 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 16:33:54 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 16:33:54 __init__.py:190] Automatically detected platform cuda.
Waiting for server on port 40000 to start...
[2025-08-25 16:34:00 TP0] Init torch distributed begin.
[2025-08-25 16:34:00 TP0] Init torch distributed begin.
[2025-08-25 16:34:01 TP0] Init torch distributed ends. mem usage=0.00 GB
[2025-08-25 16:34:01 TP0] Load weight begin. avail mem=78.73 GB
[2025-08-25 16:34:01 TP0] Init torch distributed ends. mem usage=0.00 GB
[2025-08-25 16:34:01 TP0] Load weight begin. avail mem=78.73 GB
[2025-08-25 16:34:01 TP0] The following error message 'operation scheduled before its operands' can be ignored.
[2025-08-25 16:34:01 TP0] The following error message 'operation scheduled before its operands' can be ignored.
[2025-08-25 16:34:02 TP0] Using model weights format ['*.safetensors']
[2025-08-25 16:34:02 TP0] Using model weights format ['*.safetensors']
[2025-08-25 16:34:02 TP0] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.25it/s]

[2025-08-25 16:34:03 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=75.14 GB, mem usage=3.58 GB.
[2025-08-25 16:34:03 TP0] KV Cache is allocated. #tokens: 2519168, K size: 33.63 GB, V size: 33.63 GB
[2025-08-25 16:34:03 TP0] Memory pool end. avail mem=5.76 GB
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:01<00:13,  1.03s/it]
[2025-08-25 16:34:04 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=5.15 GB
  0%|          | 0/23 [00:00<?, ?it/s]Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:02<00:13,  1.14s/it]
  4%|▍         | 1/23 [00:01<00:27,  1.23s/it]  9%|▊         | 2/23 [00:01<00:14,  1.46it/s] 13%|█▎        | 3/23 [00:01<00:10,  1.93it/s] 17%|█▋        | 4/23 [00:02<00:08,  2.25it/s]Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:03<00:13,  1.22s/it]
 22%|██▏       | 5/23 [00:02<00:07,  2.52it/s] 26%|██▌       | 6/23 [00:02<00:06,  2.74it/s] 30%|███       | 7/23 [00:03<00:05,  2.86it/s] 35%|███▍      | 8/23 [00:03<00:05,  2.95it/s] 39%|███▉      | 9/23 [00:03<00:04,  3.01it/s]Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:04<00:12,  1.28s/it]
 43%|████▎     | 10/23 [00:04<00:04,  3.15it/s] 48%|████▊     | 11/23 [00:04<00:03,  3.19it/s] 52%|█████▏    | 12/23 [00:04<00:03,  3.18it/s] 57%|█████▋    | 13/23 [00:04<00:03,  3.18it/s]Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:06<00:11,  1.30s/it]
 61%|██████    | 14/23 [00:05<00:02,  3.23it/s] 65%|██████▌   | 15/23 [00:05<00:02,  3.28it/s] 70%|██████▉   | 16/23 [00:05<00:02,  3.25it/s] 74%|███████▍  | 17/23 [00:06<00:01,  3.21it/s]Waiting for server on port 40000 to start...
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:07<00:10,  1.32s/it]
 78%|███████▊  | 18/23 [00:06<00:01,  3.14it/s] 83%|████████▎ | 19/23 [00:06<00:01,  3.24it/s] 87%|████████▋ | 20/23 [00:07<00:00,  3.21it/s] 91%|█████████▏| 21/23 [00:07<00:00,  3.21it/s] 96%|█████████▌| 22/23 [00:07<00:00,  3.19it/s]Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:08<00:09,  1.33s/it]
100%|██████████| 23/23 [00:08<00:00,  3.29it/s]100%|██████████| 23/23 [00:08<00:00,  2.86it/s]
[2025-08-25 16:34:12 TP0] Capture cuda graph end. Time elapsed: 8.05 s. avail mem=3.21 GB. mem usage=1.94 GB.
[2025-08-25 16:34:12 TP0] max_total_num_tokens=2519168, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4097, context_len=131072
[2025-08-25 16:34:13] INFO:     Started server process [2351504]
[2025-08-25 16:34:13] INFO:     Waiting for application startup.
[2025-08-25 16:34:13] INFO:     Application startup complete.
[2025-08-25 16:34:13] INFO:     Uvicorn running on http://0.0.0.0:40001 (Press CTRL+C to quit)
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:10<00:07,  1.30s/it]
[2025-08-25 16:34:14] INFO:     127.0.0.1:13684 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-08-25 16:34:14 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:11<00:06,  1.29s/it]
[2025-08-25 16:34:15] INFO:     127.0.0.1:13694 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 16:34:15] The server is fired up and ready to roll!
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:12<00:05,  1.28s/it]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:13<00:03,  1.10s/it]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:14<00:02,  1.11s/it]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:15<00:01,  1.16s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:17<00:00,  1.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:17<00:00,  1.22s/it]

[2025-08-25 16:34:20 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=17.60 GB, mem usage=61.12 GB.
[2025-08-25 16:34:20 TP0] KV Cache is allocated. #tokens: 39846, K size: 4.86 GB, V size: 4.86 GB
[2025-08-25 16:34:20 TP0] Memory pool end. avail mem=7.54 GB
Waiting for server on port 40000 to start...
[2025-08-25 16:34:20 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=6.90 GB
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:01<00:28,  1.27s/it]  9%|▊         | 2/23 [00:01<00:15,  1.34it/s] 13%|█▎        | 3/23 [00:02<00:11,  1.76it/s] 17%|█▋        | 4/23 [00:02<00:09,  2.02it/s] 22%|██▏       | 5/23 [00:02<00:08,  2.20it/s] 26%|██▌       | 6/23 [00:03<00:07,  2.35it/s] 30%|███       | 7/23 [00:03<00:06,  2.48it/s] 35%|███▍      | 8/23 [00:03<00:05,  2.56it/s] 39%|███▉      | 9/23 [00:04<00:05,  2.61it/s] 43%|████▎     | 10/23 [00:04<00:04,  2.62it/s] 48%|████▊     | 11/23 [00:04<00:04,  2.68it/s] 52%|█████▏    | 12/23 [00:05<00:04,  2.67it/s] 57%|█████▋    | 13/23 [00:05<00:03,  2.66it/s] 61%|██████    | 14/23 [00:06<00:03,  2.64it/s] 65%|██████▌   | 15/23 [00:06<00:03,  2.65it/s] 70%|██████▉   | 16/23 [00:06<00:02,  2.65it/s] 74%|███████▍  | 17/23 [00:07<00:02,  2.65it/s] 78%|███████▊  | 18/23 [00:07<00:01,  2.64it/s] 83%|████████▎ | 19/23 [00:07<00:01,  2.65it/s] 87%|████████▋ | 20/23 [00:08<00:01,  2.59it/s] 91%|█████████▏| 21/23 [00:08<00:00,  2.53it/s] 96%|█████████▌| 22/23 [00:09<00:00,  2.49it/s]100%|██████████| 23/23 [00:09<00:00,  2.47it/s]100%|██████████| 23/23 [00:09<00:00,  2.39it/s]
[2025-08-25 16:34:30 TP0] Capture cuda graph end. Time elapsed: 9.65 s. avail mem=4.93 GB. mem usage=1.97 GB.
Waiting for server on port 40000 to start...
[2025-08-25 16:34:30 TP0] max_total_num_tokens=39846, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=40960
[2025-08-25 16:34:31] INFO:     Started server process [2351503]
[2025-08-25 16:34:31] INFO:     Waiting for application startup.
[2025-08-25 16:34:31] INFO:     Application startup complete.
[2025-08-25 16:34:31] INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)
[2025-08-25 16:34:32] INFO:     127.0.0.1:64946 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-08-25 16:34:32 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 16:34:33] INFO:     127.0.0.1:64960 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 16:34:33] The server is fired up and ready to roll!
[2025-08-25 16:34:40] INFO:     127.0.0.1:24260 - "GET /get_model_info HTTP/1.1" 200 OK
Server on port 40000 is ready!
[2025-08-25 16:34:40] INFO:     127.0.0.1:40974 - "GET /get_model_info HTTP/1.1" 200 OK
Server on port 40001 is ready!
[2025-08-25 17:08:29 TP0] Prefill batch. #new-seq: 1, #new-token: 82, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 17:08:29 TP0] Prefill batch. #new-seq: 3, #new-token: 246, #cached-token: 3, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 17:08:29 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 0.07, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:29 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 901.95, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:30 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 910.69, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:30 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 854.51, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:30 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 919.86, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:30 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 908.97, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:30 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 908.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:30 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 919.42, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:31 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 896.87, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:31 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 935.01, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:31 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 902.85, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:31 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 913.00, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:31 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 903.73, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:31 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 883.30, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:32 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 906.42, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:32 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 904.37, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:32 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 897.03, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:32 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 900.74, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:32 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 909.67, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:33 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 871.53, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:33 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 874.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:33 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 905.96, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:33 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 874.46, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:33 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 906.97, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:33 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 875.72, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:34] INFO:     127.0.0.1:39468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:34] INFO:     127.0.0.1:39482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:34] INFO:     127.0.0.1:39490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:34] INFO:     127.0.0.1:39492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:34 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 17:08:34 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 246, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 17:08:34 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 566.89, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:34 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 891.29, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:34 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 884.39, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:34 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 881.75, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:34 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 887.66, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:35 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 867.99, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:35 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 842.18, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:35 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 875.71, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:35 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 890.53, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:35 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 876.61, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:36 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 879.73, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:36 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 875.84, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:36 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 882.25, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:36 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 853.42, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:36 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 876.00, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:36 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 869.32, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:37 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 855.26, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:37 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 846.55, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:37 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 879.02, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:37 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 882.90, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:37 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 887.64, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:38 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 883.46, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:38 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 869.03, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:38 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 886.68, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:38] INFO:     127.0.0.1:39468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:38 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 3, #queue-req: 0, 
[2025-08-25 17:08:38 TP0] Decode batch. #running-req: 4, #token: 3094, token usage: 0.00, gen throughput (token/s): 725.22, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:38] INFO:     127.0.0.1:39482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:38] INFO:     127.0.0.1:39490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:38] INFO:     127.0.0.1:39492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:38 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 246, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 17:08:38 TP0] Decode batch. #running-req: 4, #token: 236, token usage: 0.00, gen throughput (token/s): 676.61, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:39 TP0] Decode batch. #running-req: 4, #token: 396, token usage: 0.00, gen throughput (token/s): 858.96, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:39 TP0] Decode batch. #running-req: 4, #token: 556, token usage: 0.00, gen throughput (token/s): 879.85, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:39 TP0] Decode batch. #running-req: 4, #token: 716, token usage: 0.00, gen throughput (token/s): 896.53, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:39 TP0] Decode batch. #running-req: 4, #token: 876, token usage: 0.00, gen throughput (token/s): 892.04, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:39 TP0] Decode batch. #running-req: 4, #token: 1036, token usage: 0.00, gen throughput (token/s): 886.14, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:39 TP0] Decode batch. #running-req: 4, #token: 1196, token usage: 0.00, gen throughput (token/s): 901.08, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:40 TP0] Decode batch. #running-req: 4, #token: 1356, token usage: 0.00, gen throughput (token/s): 886.00, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:40 TP0] Decode batch. #running-req: 4, #token: 1516, token usage: 0.00, gen throughput (token/s): 885.57, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:40 TP0] Decode batch. #running-req: 4, #token: 1676, token usage: 0.00, gen throughput (token/s): 897.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:40 TP0] Decode batch. #running-req: 4, #token: 1836, token usage: 0.00, gen throughput (token/s): 865.99, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:40 TP0] Decode batch. #running-req: 4, #token: 1996, token usage: 0.00, gen throughput (token/s): 866.94, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:41 TP0] Decode batch. #running-req: 4, #token: 2156, token usage: 0.00, gen throughput (token/s): 866.70, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:41 TP0] Decode batch. #running-req: 4, #token: 2316, token usage: 0.00, gen throughput (token/s): 887.36, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:41 TP0] Decode batch. #running-req: 4, #token: 2476, token usage: 0.00, gen throughput (token/s): 915.27, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:41 TP0] Decode batch. #running-req: 4, #token: 2636, token usage: 0.00, gen throughput (token/s): 923.72, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:41 TP0] Decode batch. #running-req: 4, #token: 2796, token usage: 0.00, gen throughput (token/s): 920.75, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:41 TP0] Decode batch. #running-req: 4, #token: 2956, token usage: 0.00, gen throughput (token/s): 929.86, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:42 TP0] Decode batch. #running-req: 4, #token: 3116, token usage: 0.00, gen throughput (token/s): 950.43, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:42 TP0] Decode batch. #running-req: 4, #token: 3276, token usage: 0.00, gen throughput (token/s): 932.25, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:42 TP0] Decode batch. #running-req: 4, #token: 3436, token usage: 0.00, gen throughput (token/s): 935.80, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:42 TP0] Decode batch. #running-req: 4, #token: 3596, token usage: 0.00, gen throughput (token/s): 947.83, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:42 TP0] Decode batch. #running-req: 4, #token: 3756, token usage: 0.00, gen throughput (token/s): 934.42, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:42 TP0] Decode batch. #running-req: 4, #token: 3916, token usage: 0.00, gen throughput (token/s): 935.59, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:42] INFO:     127.0.0.1:39468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 3, #queue-req: 0, 
[2025-08-25 17:08:43 TP0] Decode batch. #running-req: 4, #token: 3074, token usage: 0.00, gen throughput (token/s): 763.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:43] INFO:     127.0.0.1:39482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:43] INFO:     127.0.0.1:39490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:43] INFO:     127.0.0.1:39492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 17:08:43 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 164, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-08-25 17:08:43 TP0] Decode batch. #running-req: 4, #token: 219, token usage: 0.00, gen throughput (token/s): 644.42, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:43 TP0] Decode batch. #running-req: 4, #token: 379, token usage: 0.00, gen throughput (token/s): 929.29, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:43 TP0] Decode batch. #running-req: 4, #token: 539, token usage: 0.00, gen throughput (token/s): 906.27, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:43 TP0] Decode batch. #running-req: 4, #token: 699, token usage: 0.00, gen throughput (token/s): 919.88, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:44 TP0] Decode batch. #running-req: 4, #token: 859, token usage: 0.00, gen throughput (token/s): 913.34, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:44 TP0] Decode batch. #running-req: 4, #token: 1019, token usage: 0.00, gen throughput (token/s): 934.31, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:44 TP0] Decode batch. #running-req: 4, #token: 1179, token usage: 0.00, gen throughput (token/s): 916.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:44 TP0] Decode batch. #running-req: 4, #token: 1339, token usage: 0.00, gen throughput (token/s): 919.32, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:44 TP0] Decode batch. #running-req: 4, #token: 1499, token usage: 0.00, gen throughput (token/s): 949.31, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:44 TP0] Decode batch. #running-req: 4, #token: 1659, token usage: 0.00, gen throughput (token/s): 928.93, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:45 TP0] Decode batch. #running-req: 4, #token: 1819, token usage: 0.00, gen throughput (token/s): 937.64, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:45 TP0] Decode batch. #running-req: 4, #token: 1979, token usage: 0.00, gen throughput (token/s): 951.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:45 TP0] Decode batch. #running-req: 4, #token: 2139, token usage: 0.00, gen throughput (token/s): 894.46, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:45 TP0] Decode batch. #running-req: 4, #token: 2299, token usage: 0.00, gen throughput (token/s): 945.32, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:45 TP0] Decode batch. #running-req: 4, #token: 2459, token usage: 0.00, gen throughput (token/s): 943.26, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:45 TP0] Decode batch. #running-req: 4, #token: 2619, token usage: 0.00, gen throughput (token/s): 938.01, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:46 TP0] Decode batch. #running-req: 4, #token: 2779, token usage: 0.00, gen throughput (token/s): 945.45, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:46 TP0] Decode batch. #running-req: 4, #token: 2939, token usage: 0.00, gen throughput (token/s): 949.35, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:46 TP0] Decode batch. #running-req: 4, #token: 3099, token usage: 0.00, gen throughput (token/s): 934.42, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:46 TP0] Decode batch. #running-req: 4, #token: 3259, token usage: 0.00, gen throughput (token/s): 935.71, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:46 TP0] Decode batch. #running-req: 4, #token: 3419, token usage: 0.00, gen throughput (token/s): 928.25, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:46 TP0] Decode batch. #running-req: 4, #token: 3579, token usage: 0.00, gen throughput (token/s): 919.20, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:47 TP0] Decode batch. #running-req: 4, #token: 3739, token usage: 0.00, gen throughput (token/s): 932.08, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:47 TP0] Decode batch. #running-req: 4, #token: 3899, token usage: 0.00, gen throughput (token/s): 922.30, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:47] INFO:     127.0.0.1:39468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:47 TP0] Prefill batch. #new-seq: 1, #new-token: 1072, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 17:08:47 TP0] Decode batch. #running-req: 3, #token: 3029, token usage: 0.00, gen throughput (token/s): 714.47, largest-len: 0, #queue-req: 0, 
[2025-08-25 17:08:47] INFO:     127.0.0.1:39482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:47] INFO:     127.0.0.1:39490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:47] INFO:     127.0.0.1:39492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 17:08:47 TP0] Prefill batch. #new-seq: 3, #new-token: 3000, #cached-token: 216, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 17:08:47] INFO:     127.0.0.1:18636 - "POST /generate HTTP/1.1" 200 OK
slurmstepd-nwonga100: error: *** JOB 11298 ON nwonga100 CANCELLED AT 2025-08-25T17:18:21 ***
