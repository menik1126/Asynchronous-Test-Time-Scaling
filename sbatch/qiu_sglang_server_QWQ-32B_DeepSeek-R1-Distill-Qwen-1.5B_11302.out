Mon Aug 25 18:00:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          Off |   00000000:4F:00.0 Off |                    0 |
| N/A   49C    P0            311W /  300W |   66763MiB /  81920MiB |     83%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          Off |   00000000:52:00.0 Off |                    0 |
| N/A   27C    P0             43W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          Off |   00000000:56:00.0 Off |                    0 |
| N/A   27C    P0             63W /  300W |   43627MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          Off |   00000000:57:00.0 Off |                    0 |
| N/A   27C    P0             44W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          Off |   00000000:D5:00.0 Off |                    0 |
| N/A   29C    P0             42W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   2223691      C   python                                      66754MiB |
|    2   N/A  N/A   2258570      C   /home/shenhui/miniconda3/bin/python         43618MiB |
+-----------------------------------------------------------------------------------------+
/home/xiongjing
Waiting for server on port 40000 to start...
INFO 08-25 18:00:41 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:00:41 __init__.py:190] Automatically detected platform cuda.
[2025-08-25 18:00:44] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=40001, mem_fraction_static=0.9, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=1, stream_interval=1, stream_output=False, random_seed=597426301, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)
[2025-08-25 18:00:44] server_args=ServerArgs(model_path='Qwen/QwQ-32B', tokenizer_path='Qwen/QwQ-32B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/QwQ-32B', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=40000, mem_fraction_static=0.9, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=1, stream_interval=1, stream_output=False, random_seed=430751180, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)
Waiting for server on port 40000 to start...
Waiting for server on port 40000 to start...
INFO 08-25 18:00:57 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:00:57 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:00:57 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:00:57 __init__.py:190] Automatically detected platform cuda.
[2025-08-25 18:01:03 TP0] Init torch distributed begin.
[2025-08-25 18:01:04 TP0] Init torch distributed ends. mem usage=0.00 GB
[2025-08-25 18:01:04 TP0] Load weight begin. avail mem=78.73 GB
[2025-08-25 18:01:04 TP0] The following error message 'operation scheduled before its operands' can be ignored.
[2025-08-25 18:01:04 TP0] Init torch distributed begin.
[2025-08-25 18:01:04 TP0] Init torch distributed ends. mem usage=0.00 GB
[2025-08-25 18:01:04 TP0] Load weight begin. avail mem=78.73 GB
[2025-08-25 18:01:05 TP0] Using model weights format ['*.safetensors']
Waiting for server on port 40000 to start...
[2025-08-25 18:01:05 TP0] The following error message 'operation scheduled before its operands' can be ignored.
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
[2025-08-25 18:01:05 TP0] Using model weights format ['*.safetensors']
[2025-08-25 18:01:06 TP0] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:11,  1.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.28it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.28it/s]

[2025-08-25 18:01:07 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=75.14 GB, mem usage=3.58 GB.
[2025-08-25 18:01:07 TP0] KV Cache is allocated. #tokens: 2519168, K size: 33.63 GB, V size: 33.63 GB
[2025-08-25 18:01:07 TP0] Memory pool end. avail mem=5.76 GB
[2025-08-25 18:01:07 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=5.15 GB
  0%|          | 0/23 [00:00<?, ?it/s]Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:02<00:12,  1.06s/it]
  4%|▍         | 1/23 [00:01<00:26,  1.19s/it]Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:03<00:11,  1.08s/it]
  9%|▊         | 2/23 [00:01<00:14,  1.50it/s] 13%|█▎        | 3/23 [00:01<00:10,  1.95it/s] 17%|█▋        | 4/23 [00:02<00:08,  2.25it/s]Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:04<00:10,  1.09s/it]
 22%|██▏       | 5/23 [00:02<00:07,  2.51it/s] 26%|██▌       | 6/23 [00:02<00:06,  2.70it/s] 30%|███       | 7/23 [00:03<00:05,  2.75it/s] 35%|███▍      | 8/23 [00:03<00:05,  2.88it/s]Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:05<00:10,  1.14s/it]
 39%|███▉      | 9/23 [00:03<00:04,  3.01it/s] 43%|████▎     | 10/23 [00:04<00:04,  3.12it/s] 48%|████▊     | 11/23 [00:04<00:03,  3.14it/s] 52%|█████▏    | 12/23 [00:04<00:03,  3.14it/s]Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:06<00:09,  1.21s/it]
 57%|█████▋    | 13/23 [00:04<00:03,  3.19it/s] 61%|██████    | 14/23 [00:05<00:02,  3.32it/s] 65%|██████▌   | 15/23 [00:05<00:02,  3.27it/s] 70%|██████▉   | 16/23 [00:05<00:02,  3.21it/s] 74%|███████▍  | 17/23 [00:06<00:02,  2.54it/s]Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:08<00:09,  1.38s/it]
 78%|███████▊  | 18/23 [00:07<00:02,  2.24it/s] 83%|████████▎ | 19/23 [00:07<00:01,  2.45it/s] 87%|████████▋ | 20/23 [00:07<00:01,  2.64it/s]Waiting for server on port 40000 to start...
 91%|█████████▏| 21/23 [00:07<00:00,  2.80it/s]Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:10<00:08,  1.40s/it]
 96%|█████████▌| 22/23 [00:08<00:00,  2.96it/s]100%|██████████| 23/23 [00:08<00:00,  3.08it/s]100%|██████████| 23/23 [00:08<00:00,  2.68it/s]
[2025-08-25 18:01:15 TP0] Capture cuda graph end. Time elapsed: 8.58 s. avail mem=3.21 GB. mem usage=1.94 GB.
[2025-08-25 18:01:16 TP0] max_total_num_tokens=2519168, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4097, context_len=131072
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:11<00:06,  1.36s/it]
[2025-08-25 18:01:16] INFO:     Started server process [2385489]
[2025-08-25 18:01:16] INFO:     Waiting for application startup.
[2025-08-25 18:01:16] INFO:     Application startup complete.
[2025-08-25 18:01:16] INFO:     Uvicorn running on http://0.0.0.0:40001 (Press CTRL+C to quit)
[2025-08-25 18:01:17] INFO:     127.0.0.1:45766 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-08-25 18:01:17 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:12<00:05,  1.34s/it]
[2025-08-25 18:01:18] INFO:     127.0.0.1:45770 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:01:18] The server is fired up and ready to roll!
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:13<00:03,  1.15s/it]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:14<00:02,  1.14s/it]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:15<00:01,  1.19s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:16<00:00,  1.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:16<00:00,  1.21s/it]

[2025-08-25 18:01:22 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=17.60 GB, mem usage=61.12 GB.
[2025-08-25 18:01:22 TP0] KV Cache is allocated. #tokens: 39846, K size: 4.86 GB, V size: 4.86 GB
[2025-08-25 18:01:22 TP0] Memory pool end. avail mem=7.54 GB
[2025-08-25 18:01:23 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=6.90 GB
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:01<00:28,  1.28s/it]  9%|▊         | 2/23 [00:01<00:15,  1.35it/s] 13%|█▎        | 3/23 [00:02<00:11,  1.76it/s]Waiting for server on port 40000 to start...
 17%|█▋        | 4/23 [00:02<00:09,  2.05it/s] 22%|██▏       | 5/23 [00:02<00:08,  2.23it/s] 26%|██▌       | 6/23 [00:03<00:07,  2.39it/s] 30%|███       | 7/23 [00:03<00:06,  2.52it/s] 35%|███▍      | 8/23 [00:03<00:05,  2.59it/s] 39%|███▉      | 9/23 [00:04<00:05,  2.63it/s] 43%|████▎     | 10/23 [00:04<00:04,  2.65it/s] 48%|████▊     | 11/23 [00:04<00:04,  2.69it/s] 52%|█████▏    | 12/23 [00:05<00:04,  2.68it/s] 57%|█████▋    | 13/23 [00:05<00:03,  2.65it/s] 61%|██████    | 14/23 [00:06<00:03,  2.65it/s] 65%|██████▌   | 15/23 [00:06<00:03,  2.64it/s] 70%|██████▉   | 16/23 [00:06<00:02,  2.64it/s] 74%|███████▍  | 17/23 [00:07<00:02,  2.63it/s] 78%|███████▊  | 18/23 [00:07<00:01,  2.63it/s] 83%|████████▎ | 19/23 [00:07<00:01,  2.65it/s] 87%|████████▋ | 20/23 [00:08<00:01,  2.58it/s] 91%|█████████▏| 21/23 [00:08<00:00,  2.52it/s] 96%|█████████▌| 22/23 [00:09<00:00,  2.48it/s]100%|██████████| 23/23 [00:09<00:00,  2.48it/s]100%|██████████| 23/23 [00:09<00:00,  2.39it/s]
[2025-08-25 18:01:32 TP0] Capture cuda graph end. Time elapsed: 9.62 s. avail mem=4.93 GB. mem usage=1.97 GB.
[2025-08-25 18:01:33 TP0] max_total_num_tokens=39846, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=2049, context_len=40960
[2025-08-25 18:01:33] INFO:     Started server process [2385488]
[2025-08-25 18:01:33] INFO:     Waiting for application startup.
[2025-08-25 18:01:33] INFO:     Application startup complete.
[2025-08-25 18:01:33] INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)
[2025-08-25 18:01:34] INFO:     127.0.0.1:53662 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-08-25 18:01:34 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:01:35] INFO:     127.0.0.1:53674 - "GET /get_model_info HTTP/1.1" 200 OK
Server on port 40000 is ready!
[2025-08-25 18:01:35] INFO:     127.0.0.1:9220 - "GET /get_model_info HTTP/1.1" 200 OK
Server on port 40001 is ready!
[2025-08-25 18:01:35] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:01:35] The server is fired up and ready to roll!
[2025-08-25 18:04:39 TP0] Prefill batch. #new-seq: 4, #new-token: 328, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:04:39 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 0.67, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:39 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 886.60, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:40 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 861.38, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:40 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 909.25, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:40 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 925.15, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:40 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 870.04, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:40 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 858.39, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:40 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 895.44, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:41 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 886.07, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:41 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 928.45, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:41 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 922.93, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:41 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 894.66, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:41 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 861.00, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:42 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 884.78, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:42 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 915.40, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:42 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 898.16, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:42 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 874.62, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:42 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 867.08, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:42 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 893.03, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:43 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 864.72, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:43 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 877.01, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:43 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 900.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:43 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 888.72, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:43 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 883.69, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:44 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 928.18, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:44] INFO:     127.0.0.1:26806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:44] INFO:     127.0.0.1:26822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:44] INFO:     127.0.0.1:26826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:44] INFO:     127.0.0.1:26840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:04:44 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 246, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:04:44 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 562.92, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:44 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 879.28, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:44 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 859.61, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:44 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 838.58, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:45 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 871.90, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:45 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 868.82, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:45 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 882.61, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:45 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 862.24, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:45 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 864.86, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:45 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 878.34, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:46 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 857.55, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:46 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 868.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:46 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 850.36, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:46 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 845.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:46 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 891.86, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:47 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 868.92, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:47 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 871.45, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:47 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 896.89, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:47 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 882.35, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:47 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 869.77, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:48 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 852.43, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:48 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 859.27, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:48 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 847.48, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:48 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 860.48, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:48 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 878.33, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:48] INFO:     127.0.0.1:26806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:48] INFO:     127.0.0.1:26822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:48] INFO:     127.0.0.1:26826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:48] INFO:     127.0.0.1:26840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:48 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:04:48 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 246, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:04:49 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 602.19, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:49 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 868.39, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:49 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 875.55, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:49 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 879.24, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:49 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 876.26, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:49 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 880.40, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:50 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 865.51, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:50 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 847.43, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:50 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 872.14, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:50 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 919.70, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:50 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 901.10, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:51 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 920.86, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:51 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 914.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:51 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 906.45, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:51 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 886.12, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:51 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 894.11, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:51 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 900.27, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:52 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 912.18, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:52 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 891.80, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:52 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 907.33, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:52 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 891.02, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:52 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 893.00, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:52 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 884.05, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:53 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 915.98, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:53 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 884.65, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:53] INFO:     127.0.0.1:26806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:53] INFO:     127.0.0.1:26822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:53] INFO:     127.0.0.1:26826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:53] INFO:     127.0.0.1:26840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:53 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:04:53 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 246, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:04:53 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 610.65, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:53 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 873.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:53 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 872.94, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:54 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 881.88, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:54 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 872.97, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:54 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 882.68, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:54 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 883.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:54 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 855.61, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:55 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 848.64, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:55 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 878.25, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:55 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 881.00, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:55 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 887.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:55 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 866.56, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:55 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 883.55, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:56 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 881.61, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:56 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 891.22, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:56 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 875.98, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:56 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 814.35, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:56 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 859.89, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:57 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 879.11, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:57 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 866.78, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:57 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 870.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:57 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 878.59, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:57 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 842.63, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:58 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 882.83, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:04:58] INFO:     127.0.0.1:26806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:58] INFO:     127.0.0.1:26822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:58] INFO:     127.0.0.1:26826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:58] INFO:     127.0.0.1:26840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:04:58 TP0] Prefill batch. #new-seq: 3, #new-token: 3216, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:04:58 TP0] Prefill batch. #new-seq: 1, #new-token: 1072, #cached-token: 0, token usage: 0.08, #running-req: 3, #queue-req: 0, 
[2025-08-25 18:04:59] INFO:     127.0.0.1:7226 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:04:59] INFO:     127.0.0.1:7234 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:04:59] INFO:     127.0.0.1:7244 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:04:59 TP0] Prefill batch. #new-seq: 1, #new-token: 1000, #cached-token: 72, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:04:59 TP0] Prefill batch. #new-seq: 2, #new-token: 2000, #cached-token: 144, token usage: 0.03, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:04:59] INFO:     127.0.0.1:7260 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:05:00 TP0] Prefill batch. #new-seq: 1, #new-token: 1000, #cached-token: 72, token usage: 0.05, #running-req: 3, #queue-req: 0, 
[2025-08-25 18:05:00] INFO:     127.0.0.1:7226 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:05:00] INFO:     127.0.0.1:7234 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:05:00] INFO:     127.0.0.1:7244 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:05:00 TP0] Prefill batch. #new-seq: 3, #new-token: 3000, #cached-token: 216, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:05:00] INFO:     127.0.0.1:7260 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:05:01 TP0] TpModelWorkerClient hit an exception: Traceback (most recent call last):
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 109, in forward_thread_func
    self.forward_thread_func_()
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 140, in forward_thread_func_
    logits_output, next_token_ids = self.worker.forward_batch_generation(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 172, in forward_batch_generation
    logits_output = self.model_runner.forward(forward_batch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 909, in forward
    return self.forward_extend(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 870, in forward_extend
    return self.model.forward(
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/models/qwen2.py", line 376, in forward
    return self.logits_processor(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/layers/logits_processor.py", line 338, in forward
    input_logprobs = logits[input_logprob_indices]
                     ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.70 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.47 GiB is free. Including non-PyTorch memory, this process has 77.66 GiB memory in use. Of the allocated memory 75.17 GiB is allocated by PyTorch, with 521.14 MiB allocated in private pools (e.g., CUDA Graphs), and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-08-25 18:05:01] Received sigquit from a child process. It usually means the child failed.
/var/lib/slurm/slurmd/job11302/slurm_script: line 65: 2385488 Killed                  CUDA_VISIBLE_DEVICES=3 python3 -m sglang.launch_server --model-path Qwen/QwQ-32B --tp 1 --mem-fraction-static 0.9 --host 0.0.0.0 --port 40000
[2025-08-25 18:05:08 TP0] Prefill batch. #new-seq: 4, #new-token: 648, #cached-token: 200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:05:08 TP0] Decode batch. #running-req: 4, #token: 344, token usage: 0.00, gen throughput (token/s): 15.54, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:08 TP0] Decode batch. #running-req: 4, #token: 504, token usage: 0.00, gen throughput (token/s): 987.75, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:08 TP0] Decode batch. #running-req: 4, #token: 664, token usage: 0.00, gen throughput (token/s): 982.17, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:08 TP0] Decode batch. #running-req: 4, #token: 824, token usage: 0.00, gen throughput (token/s): 982.01, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:08 TP0] Decode batch. #running-req: 4, #token: 984, token usage: 0.00, gen throughput (token/s): 986.41, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:09 TP0] Decode batch. #running-req: 4, #token: 1144, token usage: 0.00, gen throughput (token/s): 982.81, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:09 TP0] Decode batch. #running-req: 4, #token: 1304, token usage: 0.00, gen throughput (token/s): 984.81, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:09 TP0] Decode batch. #running-req: 4, #token: 1464, token usage: 0.00, gen throughput (token/s): 978.85, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:09 TP0] Decode batch. #running-req: 4, #token: 1624, token usage: 0.00, gen throughput (token/s): 983.77, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:09 TP0] Decode batch. #running-req: 4, #token: 1784, token usage: 0.00, gen throughput (token/s): 983.69, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:09 TP0] Decode batch. #running-req: 4, #token: 1944, token usage: 0.00, gen throughput (token/s): 982.54, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:10 TP0] Decode batch. #running-req: 4, #token: 2104, token usage: 0.00, gen throughput (token/s): 977.05, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:10 TP0] Decode batch. #running-req: 4, #token: 2264, token usage: 0.00, gen throughput (token/s): 980.93, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:10 TP0] Decode batch. #running-req: 4, #token: 2424, token usage: 0.00, gen throughput (token/s): 982.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:10 TP0] Decode batch. #running-req: 4, #token: 2584, token usage: 0.00, gen throughput (token/s): 982.19, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:10 TP0] Decode batch. #running-req: 4, #token: 2744, token usage: 0.00, gen throughput (token/s): 976.76, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:10 TP0] Decode batch. #running-req: 4, #token: 2904, token usage: 0.00, gen throughput (token/s): 975.70, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:11 TP0] Decode batch. #running-req: 4, #token: 3064, token usage: 0.00, gen throughput (token/s): 976.67, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:11 TP0] Decode batch. #running-req: 4, #token: 3224, token usage: 0.00, gen throughput (token/s): 980.95, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:11 TP0] Decode batch. #running-req: 4, #token: 3384, token usage: 0.00, gen throughput (token/s): 980.65, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:11 TP0] Decode batch. #running-req: 4, #token: 3544, token usage: 0.00, gen throughput (token/s): 973.27, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:11 TP0] Decode batch. #running-req: 4, #token: 3704, token usage: 0.00, gen throughput (token/s): 971.37, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:11 TP0] Decode batch. #running-req: 4, #token: 3864, token usage: 0.00, gen throughput (token/s): 971.72, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:12 TP0] Decode batch. #running-req: 4, #token: 4024, token usage: 0.00, gen throughput (token/s): 976.00, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:12 TP0] Decode batch. #running-req: 4, #token: 4184, token usage: 0.00, gen throughput (token/s): 974.49, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:12] INFO:     127.0.0.1:62764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:12] INFO:     127.0.0.1:62776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:12] INFO:     127.0.0.1:62788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:12] INFO:     127.0.0.1:62798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:12 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 422, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:05:12 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 422, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-08-25 18:05:12 TP0] Decode batch. #running-req: 4, #token: 344, token usage: 0.00, gen throughput (token/s): 711.13, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:12 TP0] Decode batch. #running-req: 4, #token: 504, token usage: 0.00, gen throughput (token/s): 986.39, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:12 TP0] Decode batch. #running-req: 4, #token: 664, token usage: 0.00, gen throughput (token/s): 986.38, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:12 TP0] Decode batch. #running-req: 4, #token: 824, token usage: 0.00, gen throughput (token/s): 985.97, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:13 TP0] Decode batch. #running-req: 4, #token: 984, token usage: 0.00, gen throughput (token/s): 985.77, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:13 TP0] Decode batch. #running-req: 4, #token: 1144, token usage: 0.00, gen throughput (token/s): 984.74, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:13 TP0] Decode batch. #running-req: 4, #token: 1304, token usage: 0.00, gen throughput (token/s): 985.21, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:13 TP0] Decode batch. #running-req: 4, #token: 1464, token usage: 0.00, gen throughput (token/s): 979.19, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:13 TP0] Decode batch. #running-req: 4, #token: 1624, token usage: 0.00, gen throughput (token/s): 977.77, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:13 TP0] Decode batch. #running-req: 4, #token: 1784, token usage: 0.00, gen throughput (token/s): 983.76, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:14 TP0] Decode batch. #running-req: 4, #token: 1944, token usage: 0.00, gen throughput (token/s): 982.49, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:14 TP0] Decode batch. #running-req: 4, #token: 2104, token usage: 0.00, gen throughput (token/s): 982.65, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:14 TP0] Decode batch. #running-req: 4, #token: 2264, token usage: 0.00, gen throughput (token/s): 978.05, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:14 TP0] Decode batch. #running-req: 4, #token: 2424, token usage: 0.00, gen throughput (token/s): 982.73, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:14 TP0] Decode batch. #running-req: 4, #token: 2584, token usage: 0.00, gen throughput (token/s): 982.57, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:14 TP0] Decode batch. #running-req: 4, #token: 2744, token usage: 0.00, gen throughput (token/s): 981.70, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:15 TP0] Decode batch. #running-req: 4, #token: 2904, token usage: 0.00, gen throughput (token/s): 981.67, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:15 TP0] Decode batch. #running-req: 4, #token: 3064, token usage: 0.00, gen throughput (token/s): 976.25, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:15 TP0] Decode batch. #running-req: 4, #token: 3224, token usage: 0.00, gen throughput (token/s): 980.57, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:15 TP0] Decode batch. #running-req: 4, #token: 3384, token usage: 0.00, gen throughput (token/s): 978.40, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:15 TP0] Decode batch. #running-req: 4, #token: 3544, token usage: 0.00, gen throughput (token/s): 978.28, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:15 TP0] Decode batch. #running-req: 4, #token: 3704, token usage: 0.00, gen throughput (token/s): 977.19, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:16 TP0] Decode batch. #running-req: 4, #token: 3864, token usage: 0.00, gen throughput (token/s): 970.85, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:16 TP0] Decode batch. #running-req: 4, #token: 4024, token usage: 0.00, gen throughput (token/s): 970.46, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:16 TP0] Decode batch. #running-req: 4, #token: 4184, token usage: 0.00, gen throughput (token/s): 975.34, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:16] INFO:     127.0.0.1:62798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:16] INFO:     127.0.0.1:62764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:16] INFO:     127.0.0.1:62776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:16] INFO:     127.0.0.1:62788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:16 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 211, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:05:16 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 633, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:05:16 TP0] Decode batch. #running-req: 4, #token: 344, token usage: 0.00, gen throughput (token/s): 711.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:16 TP0] Decode batch. #running-req: 4, #token: 504, token usage: 0.00, gen throughput (token/s): 987.35, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:16 TP0] Decode batch. #running-req: 4, #token: 664, token usage: 0.00, gen throughput (token/s): 981.53, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:17 TP0] Decode batch. #running-req: 4, #token: 824, token usage: 0.00, gen throughput (token/s): 985.71, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:17 TP0] Decode batch. #running-req: 4, #token: 984, token usage: 0.00, gen throughput (token/s): 985.36, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:17 TP0] Decode batch. #running-req: 4, #token: 1144, token usage: 0.00, gen throughput (token/s): 979.47, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:17 TP0] Decode batch. #running-req: 4, #token: 1304, token usage: 0.00, gen throughput (token/s): 978.35, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:17 TP0] Decode batch. #running-req: 4, #token: 1464, token usage: 0.00, gen throughput (token/s): 979.39, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:17 TP0] Decode batch. #running-req: 4, #token: 1624, token usage: 0.00, gen throughput (token/s): 984.23, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:18 TP0] Decode batch. #running-req: 4, #token: 1784, token usage: 0.00, gen throughput (token/s): 983.49, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:18 TP0] Decode batch. #running-req: 4, #token: 1944, token usage: 0.00, gen throughput (token/s): 982.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:18 TP0] Decode batch. #running-req: 4, #token: 2104, token usage: 0.00, gen throughput (token/s): 982.57, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:18 TP0] Decode batch. #running-req: 4, #token: 2264, token usage: 0.00, gen throughput (token/s): 983.06, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:18 TP0] Decode batch. #running-req: 4, #token: 2424, token usage: 0.00, gen throughput (token/s): 976.97, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:18 TP0] Decode batch. #running-req: 4, #token: 2584, token usage: 0.00, gen throughput (token/s): 981.95, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:19 TP0] Decode batch. #running-req: 4, #token: 2744, token usage: 0.00, gen throughput (token/s): 979.27, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:19 TP0] Decode batch. #running-req: 4, #token: 2904, token usage: 0.00, gen throughput (token/s): 981.54, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:19 TP0] Decode batch. #running-req: 4, #token: 3064, token usage: 0.00, gen throughput (token/s): 980.94, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:19 TP0] Decode batch. #running-req: 4, #token: 3224, token usage: 0.00, gen throughput (token/s): 980.73, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:19 TP0] Decode batch. #running-req: 4, #token: 3384, token usage: 0.00, gen throughput (token/s): 980.63, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:19 TP0] Decode batch. #running-req: 4, #token: 3544, token usage: 0.00, gen throughput (token/s): 973.43, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:20 TP0] Decode batch. #running-req: 4, #token: 3704, token usage: 0.00, gen throughput (token/s): 971.36, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:20 TP0] Decode batch. #running-req: 4, #token: 3864, token usage: 0.00, gen throughput (token/s): 974.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:20 TP0] Decode batch. #running-req: 4, #token: 4024, token usage: 0.00, gen throughput (token/s): 975.44, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:20 TP0] Decode batch. #running-req: 4, #token: 4184, token usage: 0.00, gen throughput (token/s): 975.57, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:20] INFO:     127.0.0.1:62798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:20] INFO:     127.0.0.1:62764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:20] INFO:     127.0.0.1:62776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:20] INFO:     127.0.0.1:62788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:20 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 211, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:05:20 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 633, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:05:20 TP0] Decode batch. #running-req: 4, #token: 344, token usage: 0.00, gen throughput (token/s): 708.61, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:20 TP0] Decode batch. #running-req: 4, #token: 504, token usage: 0.00, gen throughput (token/s): 987.34, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:21 TP0] Decode batch. #running-req: 4, #token: 664, token usage: 0.00, gen throughput (token/s): 987.27, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:21 TP0] Decode batch. #running-req: 4, #token: 824, token usage: 0.00, gen throughput (token/s): 986.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:21 TP0] Decode batch. #running-req: 4, #token: 984, token usage: 0.00, gen throughput (token/s): 985.56, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:21 TP0] Decode batch. #running-req: 4, #token: 1144, token usage: 0.00, gen throughput (token/s): 985.16, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:21 TP0] Decode batch. #running-req: 4, #token: 1304, token usage: 0.00, gen throughput (token/s): 979.14, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:21 TP0] Decode batch. #running-req: 4, #token: 1464, token usage: 0.00, gen throughput (token/s): 979.74, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:22 TP0] Decode batch. #running-req: 4, #token: 1624, token usage: 0.00, gen throughput (token/s): 983.85, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:22 TP0] Decode batch. #running-req: 4, #token: 1784, token usage: 0.00, gen throughput (token/s): 983.68, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:22 TP0] Decode batch. #running-req: 4, #token: 1944, token usage: 0.00, gen throughput (token/s): 983.82, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:22 TP0] Decode batch. #running-req: 4, #token: 2104, token usage: 0.00, gen throughput (token/s): 976.90, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:22 TP0] Decode batch. #running-req: 4, #token: 2264, token usage: 0.00, gen throughput (token/s): 977.76, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:22 TP0] Decode batch. #running-req: 4, #token: 2424, token usage: 0.00, gen throughput (token/s): 982.96, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:23 TP0] Decode batch. #running-req: 4, #token: 2584, token usage: 0.00, gen throughput (token/s): 982.24, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:23 TP0] Decode batch. #running-req: 4, #token: 2744, token usage: 0.00, gen throughput (token/s): 977.60, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:23 TP0] Decode batch. #running-req: 4, #token: 2904, token usage: 0.00, gen throughput (token/s): 976.68, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:23 TP0] Decode batch. #running-req: 4, #token: 3064, token usage: 0.00, gen throughput (token/s): 976.34, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:23 TP0] Decode batch. #running-req: 4, #token: 3224, token usage: 0.00, gen throughput (token/s): 980.95, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:23 TP0] Decode batch. #running-req: 4, #token: 3384, token usage: 0.00, gen throughput (token/s): 981.10, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:23 TP0] Decode batch. #running-req: 4, #token: 3544, token usage: 0.00, gen throughput (token/s): 979.22, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:24 TP0] Decode batch. #running-req: 4, #token: 3704, token usage: 0.00, gen throughput (token/s): 977.44, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:24 TP0] Decode batch. #running-req: 4, #token: 3864, token usage: 0.00, gen throughput (token/s): 976.58, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:24 TP0] Decode batch. #running-req: 4, #token: 4024, token usage: 0.00, gen throughput (token/s): 975.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:24 TP0] Decode batch. #running-req: 4, #token: 4184, token usage: 0.00, gen throughput (token/s): 975.64, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:05:24] INFO:     127.0.0.1:62776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:24] INFO:     127.0.0.1:62798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:24] INFO:     127.0.0.1:62764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:05:24] INFO:     127.0.0.1:62788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
slurmstepd-nwonga100: error: *** JOB 11302 ON nwonga100 CANCELLED AT 2025-08-25T18:06:15 ***
