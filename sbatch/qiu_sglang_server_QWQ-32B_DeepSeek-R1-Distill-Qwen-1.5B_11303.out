Mon Aug 25 18:06:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          Off |   00000000:4F:00.0 Off |                    0 |
| N/A   49C    P0            298W /  300W |   66763MiB /  81920MiB |     99%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          Off |   00000000:52:00.0 Off |                    0 |
| N/A   28C    P0             43W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          Off |   00000000:56:00.0 Off |                    0 |
| N/A   27C    P0             63W /  300W |   43627MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          Off |   00000000:57:00.0 Off |                    0 |
| N/A   28C    P0             44W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A100 80GB PCIe          Off |   00000000:D5:00.0 Off |                    0 |
| N/A   30C    P0             43W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   2223691      C   python                                      66754MiB |
|    2   N/A  N/A   2258570      C   /home/shenhui/miniconda3/bin/python         43618MiB |
+-----------------------------------------------------------------------------------------+
/home/xiongjing
Waiting for server on port 40000 to start...
INFO 08-25 18:07:03 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:07:03 __init__.py:190] Automatically detected platform cuda.
[2025-08-25 18:07:05] server_args=ServerArgs(model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', tokenizer_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=40001, mem_fraction_static=0.9, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=1, stream_interval=1, stream_output=False, random_seed=956499288, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)
[2025-08-25 18:07:05] server_args=ServerArgs(model_path='Qwen/QwQ-32B', tokenizer_path='Qwen/QwQ-32B', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/QwQ-32B', chat_template=None, is_embedding=False, revision=None, host='0.0.0.0', port=40000, mem_fraction_static=0.9, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, tp_size=2, stream_interval=1, stream_output=False, random_seed=410048601, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=5, speculative_eagle_topk=4, speculative_num_draft_tokens=8, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False, flashinfer_mla_disable_ragged=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False)
Waiting for server on port 40000 to start...
Waiting for server on port 40000 to start...
INFO 08-25 18:07:21 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:07:22 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:07:22 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:07:22 __init__.py:190] Automatically detected platform cuda.
INFO 08-25 18:07:22 __init__.py:190] Automatically detected platform cuda.
Waiting for server on port 40000 to start...
[2025-08-25 18:07:29 TP0] Init torch distributed begin.
[2025-08-25 18:07:30 TP0] Init torch distributed ends. mem usage=0.00 GB
[2025-08-25 18:07:30 TP0] Load weight begin. avail mem=78.73 GB
[2025-08-25 18:07:30 TP0] The following error message 'operation scheduled before its operands' can be ignored.
[2025-08-25 18:07:30 TP0] Init torch distributed begin.
[2025-08-25 18:07:31 TP0] Using model weights format ['*.safetensors']
[2025-08-25 18:07:31 TP0] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.65it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.65it/s]

[2025-08-25 18:07:32 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=75.14 GB, mem usage=3.58 GB.
[2025-08-25 18:07:32 TP0] KV Cache is allocated. #tokens: 2519168, K size: 33.63 GB, V size: 33.63 GB
[2025-08-25 18:07:32 TP0] Memory pool end. avail mem=5.76 GB
[2025-08-25 18:07:32 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=5.15 GB
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:19,  1.11it/s]  9%|▊         | 2/23 [00:01<00:10,  2.02it/s] 13%|█▎        | 3/23 [00:01<00:07,  2.71it/s] 17%|█▋        | 4/23 [00:01<00:05,  3.17it/s] 22%|██▏       | 5/23 [00:01<00:05,  3.53it/s] 26%|██▌       | 6/23 [00:02<00:04,  3.81it/s] 30%|███       | 7/23 [00:02<00:03,  4.02it/s] 35%|███▍      | 8/23 [00:02<00:03,  4.23it/s] 39%|███▉      | 9/23 [00:02<00:03,  4.41it/s] 43%|████▎     | 10/23 [00:02<00:02,  4.48it/s] 48%|████▊     | 11/23 [00:03<00:02,  4.50it/s] 52%|█████▏    | 12/23 [00:03<00:02,  4.51it/s] 57%|█████▋    | 13/23 [00:03<00:02,  4.52it/s] 61%|██████    | 14/23 [00:03<00:01,  4.58it/s] 65%|██████▌   | 15/23 [00:03<00:01,  4.65it/s]Waiting for server on port 40000 to start...
 70%|██████▉   | 16/23 [00:04<00:01,  4.69it/s] 74%|███████▍  | 17/23 [00:04<00:01,  4.72it/s] 78%|███████▊  | 18/23 [00:04<00:01,  4.66it/s] 83%|████████▎ | 19/23 [00:04<00:00,  4.63it/s] 87%|████████▋ | 20/23 [00:05<00:00,  4.59it/s] 91%|█████████▏| 21/23 [00:05<00:00,  4.57it/s] 96%|█████████▌| 22/23 [00:05<00:00,  4.55it/s]100%|██████████| 23/23 [00:05<00:00,  4.62it/s]100%|██████████| 23/23 [00:05<00:00,  4.05it/s]
[2025-08-25 18:07:38 TP0] Capture cuda graph end. Time elapsed: 5.68 s. avail mem=3.21 GB. mem usage=1.94 GB.
[2025-08-25 18:07:38 TP0] max_total_num_tokens=2519168, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4097, context_len=131072
[2025-08-25 18:07:38] INFO:     Started server process [2388689]
[2025-08-25 18:07:38] INFO:     Waiting for application startup.
[2025-08-25 18:07:38] INFO:     Application startup complete.
[2025-08-25 18:07:38] INFO:     Uvicorn running on http://0.0.0.0:40001 (Press CTRL+C to quit)
[2025-08-25 18:07:39] INFO:     127.0.0.1:62262 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-08-25 18:07:39 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:07:40] INFO:     127.0.0.1:62272 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:07:40] The server is fired up and ready to roll!
[2025-08-25 18:07:40 TP1] Init torch distributed begin.
[2025-08-25 18:07:40 TP0] sglang is using nccl==2.21.5
[2025-08-25 18:07:40 TP1] sglang is using nccl==2.21.5
[2025-08-25 18:07:41 TP0] Init torch distributed ends. mem usage=0.25 GB
[2025-08-25 18:07:41 TP0] Load weight begin. avail mem=78.48 GB
[2025-08-25 18:07:41 TP1] Init torch distributed ends. mem usage=0.25 GB
[2025-08-25 18:07:41 TP1] Load weight begin. avail mem=78.48 GB
[2025-08-25 18:07:42 TP0] The following error message 'operation scheduled before its operands' can be ignored.
[2025-08-25 18:07:42 TP1] The following error message 'operation scheduled before its operands' can be ignored.
[2025-08-25 18:07:43 TP0] Using model weights format ['*.safetensors']
[2025-08-25 18:07:43 TP1] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:01<00:15,  1.21s/it]
Waiting for server on port 40000 to start...
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:02<00:17,  1.48s/it]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:04<00:17,  1.59s/it]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:06<00:16,  1.63s/it]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:07<00:14,  1.62s/it]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:09<00:13,  1.73s/it]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:11<00:12,  1.73s/it]
Waiting for server on port 40000 to start...
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:13<00:10,  1.71s/it]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:14<00:08,  1.71s/it]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:16<00:07,  1.75s/it]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:17<00:04,  1.41s/it]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:19<00:03,  1.51s/it]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:20<00:01,  1.57s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:22<00:00,  1.58s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:22<00:00,  1.60s/it]

[2025-08-25 18:08:06 TP1] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=47.75 GB, mem usage=30.72 GB.
Waiting for server on port 40000 to start...
[2025-08-25 18:08:06 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=47.75 GB, mem usage=30.72 GB.
[2025-08-25 18:08:06 TP0] KV Cache is allocated. #tokens: 326885, K size: 19.95 GB, V size: 19.95 GB
[2025-08-25 18:08:06 TP0] Memory pool end. avail mem=7.13 GB
[2025-08-25 18:08:06 TP1] KV Cache is allocated. #tokens: 326885, K size: 19.95 GB, V size: 19.95 GB
[2025-08-25 18:08:06 TP1] Memory pool end. avail mem=7.13 GB
[2025-08-25 18:08:07 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=6.51 GB
  0%|          | 0/23 [00:00<?, ?it/s][2025-08-25 18:08:07 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=6.51 GB
  4%|▍         | 1/23 [00:02<01:04,  2.93s/it]  9%|▊         | 2/23 [00:03<00:35,  1.67s/it] 13%|█▎        | 3/23 [00:04<00:29,  1.47s/it] 17%|█▋        | 4/23 [00:05<00:21,  1.15s/it] 22%|██▏       | 5/23 [00:06<00:18,  1.01s/it] 26%|██▌       | 6/23 [00:07<00:15,  1.10it/s] 30%|███       | 7/23 [00:07<00:13,  1.23it/s] 35%|███▍      | 8/23 [00:08<00:11,  1.32it/s] 39%|███▉      | 9/23 [00:08<00:10,  1.38it/s]Waiting for server on port 40000 to start...
 43%|████▎     | 10/23 [00:09<00:09,  1.37it/s] 48%|████▊     | 11/23 [00:10<00:08,  1.46it/s] 52%|█████▏    | 12/23 [00:11<00:07,  1.39it/s] 57%|█████▋    | 13/23 [00:11<00:06,  1.43it/s] 61%|██████    | 14/23 [00:12<00:06,  1.48it/s] 65%|██████▌   | 15/23 [00:12<00:05,  1.55it/s] 70%|██████▉   | 16/23 [00:13<00:04,  1.56it/s] 74%|███████▍  | 17/23 [00:14<00:03,  1.55it/s] 78%|███████▊  | 18/23 [00:14<00:03,  1.51it/s] 83%|████████▎ | 19/23 [00:15<00:02,  1.60it/s] 87%|████████▋ | 20/23 [00:16<00:01,  1.51it/s] 91%|█████████▏| 21/23 [00:16<00:01,  1.55it/s] 96%|█████████▌| 22/23 [00:17<00:00,  1.51it/s][2025-08-25 18:08:25 TP1] Registering 2967 cuda graph addresses
100%|██████████| 23/23 [00:18<00:00,  1.52it/s]100%|██████████| 23/23 [00:18<00:00,  1.27it/s]
[2025-08-25 18:08:25 TP0] Registering 2967 cuda graph addresses
[2025-08-25 18:08:25 TP1] Capture cuda graph end. Time elapsed: 18.23 s. avail mem=4.09 GB. mem usage=2.42 GB.
[2025-08-25 18:08:25 TP0] Capture cuda graph end. Time elapsed: 18.26 s. avail mem=4.09 GB. mem usage=2.42 GB.
[2025-08-25 18:08:26 TP1] max_total_num_tokens=326885, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4087, context_len=40960
[2025-08-25 18:08:26 TP0] max_total_num_tokens=326885, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4087, context_len=40960
[2025-08-25 18:08:26] INFO:     Started server process [2388688]
[2025-08-25 18:08:26] INFO:     Waiting for application startup.
[2025-08-25 18:08:26] INFO:     Application startup complete.
[2025-08-25 18:08:26] INFO:     Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit)
[2025-08-25 18:08:26] INFO:     127.0.0.1:1506 - "GET /get_model_info HTTP/1.1" 200 OK
Server on port 40000 is ready!
[2025-08-25 18:08:26] INFO:     127.0.0.1:39874 - "GET /get_model_info HTTP/1.1" 200 OK
Server on port 40001 is ready!
[2025-08-25 18:08:27] INFO:     127.0.0.1:1510 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-08-25 18:08:27 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:08:33] INFO:     127.0.0.1:1524 - "POST /generate HTTP/1.1" 200 OK
[2025-08-25 18:08:33] The server is fired up and ready to roll!
[2025-08-25 18:09:59 TP0] Prefill batch. #new-seq: 4, #new-token: 328, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:09:59 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 0.97, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:09:59 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 879.23, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:09:59 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 886.99, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:00 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 867.97, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:00 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 893.83, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:00 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 873.81, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:00 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 878.82, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:00 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 864.48, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:00 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 866.02, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:01 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 886.81, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:01 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 867.71, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:01 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 856.54, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:01 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 837.11, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:01 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 852.39, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:02 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 853.95, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:02 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 860.40, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:02 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 834.56, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:02 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 846.11, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:02 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 863.86, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:03 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 864.65, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:03 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 848.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:03 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 851.93, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:03 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 836.64, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:03 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 844.73, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:03 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 858.83, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:04] INFO:     127.0.0.1:62140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:04] INFO:     127.0.0.1:62156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:04] INFO:     127.0.0.1:62128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:04] INFO:     127.0.0.1:62166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:04 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:10:04 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 246, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:10:04 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 558.41, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:04 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 863.27, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:04 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 831.21, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:04 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 857.66, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:05 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 885.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:05 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 868.08, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:05 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 862.72, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:05 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 850.34, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:05 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 847.35, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:05 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 860.15, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:06 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 865.99, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:06 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 852.93, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:06 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 842.34, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:06 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 828.03, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:06 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 860.11, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:07 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 853.44, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:07 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 847.86, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:07 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 860.78, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:07 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 852.25, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:07 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 863.84, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:08 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 851.19, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:08 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 856.24, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:08 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 849.05, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:08 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 845.05, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:08 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 850.42, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:08] INFO:     127.0.0.1:62128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:08] INFO:     127.0.0.1:62140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:08] INFO:     127.0.0.1:62156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:08] INFO:     127.0.0.1:62166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:08 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:10:08 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 246, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:10:09 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 576.24, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:09 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 862.12, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:09 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 836.87, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:09 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 844.49, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:09 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 860.81, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:09 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 831.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:10 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 842.57, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:10 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 836.07, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:10 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 835.61, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:10 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 860.90, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:10 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 850.94, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:11 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 836.69, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:11 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 859.31, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:11 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 863.33, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:11 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 863.41, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:11 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 870.33, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:12 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 852.57, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:12 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 842.25, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:12 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 902.87, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:12 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 880.60, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:12 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 853.69, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:12 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 850.57, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:13 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 852.39, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:13 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 856.62, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:13 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 865.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:13] INFO:     127.0.0.1:62128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:13] INFO:     127.0.0.1:62140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:13] INFO:     127.0.0.1:62156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:13] INFO:     127.0.0.1:62166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:13 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 82, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:10:13 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 246, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-08-25 18:10:13 TP0] Decode batch. #running-req: 4, #token: 215, token usage: 0.00, gen throughput (token/s): 581.79, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:14 TP0] Decode batch. #running-req: 4, #token: 375, token usage: 0.00, gen throughput (token/s): 860.14, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:14 TP0] Decode batch. #running-req: 4, #token: 535, token usage: 0.00, gen throughput (token/s): 842.91, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:14 TP0] Decode batch. #running-req: 4, #token: 695, token usage: 0.00, gen throughput (token/s): 847.07, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:14 TP0] Decode batch. #running-req: 4, #token: 855, token usage: 0.00, gen throughput (token/s): 862.73, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:14 TP0] Decode batch. #running-req: 4, #token: 1015, token usage: 0.00, gen throughput (token/s): 839.94, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:14 TP0] Decode batch. #running-req: 4, #token: 1175, token usage: 0.00, gen throughput (token/s): 837.05, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:15 TP0] Decode batch. #running-req: 4, #token: 1335, token usage: 0.00, gen throughput (token/s): 844.99, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:15 TP0] Decode batch. #running-req: 4, #token: 1495, token usage: 0.00, gen throughput (token/s): 852.26, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:15 TP0] Decode batch. #running-req: 4, #token: 1655, token usage: 0.00, gen throughput (token/s): 852.72, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:15 TP0] Decode batch. #running-req: 4, #token: 1815, token usage: 0.00, gen throughput (token/s): 847.44, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:15 TP0] Decode batch. #running-req: 4, #token: 1975, token usage: 0.00, gen throughput (token/s): 834.76, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:16 TP0] Decode batch. #running-req: 4, #token: 2135, token usage: 0.00, gen throughput (token/s): 844.87, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:16 TP0] Decode batch. #running-req: 4, #token: 2295, token usage: 0.00, gen throughput (token/s): 853.39, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:16 TP0] Decode batch. #running-req: 4, #token: 2455, token usage: 0.00, gen throughput (token/s): 851.31, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:16 TP0] Decode batch. #running-req: 4, #token: 2615, token usage: 0.00, gen throughput (token/s): 877.98, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:16 TP0] Decode batch. #running-req: 4, #token: 2775, token usage: 0.00, gen throughput (token/s): 860.44, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:17 TP0] Decode batch. #running-req: 4, #token: 2935, token usage: 0.00, gen throughput (token/s): 867.29, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:17 TP0] Decode batch. #running-req: 4, #token: 3095, token usage: 0.00, gen throughput (token/s): 872.50, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:17 TP0] Decode batch. #running-req: 4, #token: 3255, token usage: 0.00, gen throughput (token/s): 860.98, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:17 TP0] Decode batch. #running-req: 4, #token: 3415, token usage: 0.00, gen throughput (token/s): 853.81, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:17 TP0] Decode batch. #running-req: 4, #token: 3575, token usage: 0.00, gen throughput (token/s): 837.60, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:17 TP0] Decode batch. #running-req: 4, #token: 3735, token usage: 0.00, gen throughput (token/s): 855.12, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:18 TP0] Decode batch. #running-req: 4, #token: 3895, token usage: 0.00, gen throughput (token/s): 851.30, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:18 TP0] Decode batch. #running-req: 4, #token: 4055, token usage: 0.00, gen throughput (token/s): 848.63, largest-len: 0, #queue-req: 0, 
[2025-08-25 18:10:18] INFO:     127.0.0.1:62128 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:18] INFO:     127.0.0.1:62140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:18] INFO:     127.0.0.1:62156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:18] INFO:     127.0.0.1:62166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-08-25 18:10:18 TP0] Prefill batch. #new-seq: 4, #new-token: 4288, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-08-25 18:10:19 TP0] TpModelWorkerClient hit an exception: Traceback (most recent call last):
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 109, in forward_thread_func
    self.forward_thread_func_()
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py", line 140, in forward_thread_func_
    logits_output, next_token_ids = self.worker.forward_batch_generation(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 172, in forward_batch_generation
    logits_output = self.model_runner.forward(forward_batch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 909, in forward
    return self.forward_extend(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 870, in forward_extend
    return self.model.forward(
           ^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/models/qwen2.py", line 376, in forward
    return self.logits_processor(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/xiongjing/resource_dir/qiu/.sglang/lib/python3.12/site-packages/sglang/srt/layers/logits_processor.py", line 338, in forward
    input_logprobs = logits[input_logprob_indices]
                     ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.27 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.71 GiB is free. Including non-PyTorch memory, this process has 77.42 GiB memory in use. Of the allocated memory 76.43 GiB is allocated by PyTorch, with 959.14 MiB allocated in private pools (e.g., CUDA Graphs), and 97.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-08-25 18:10:19] Received sigquit from a child process. It usually means the child failed.
/var/lib/slurm/slurmd/job11303/slurm_script: line 65: 2388688 Killed                  CUDA_VISIBLE_DEVICES=3,4 python3 -m sglang.launch_server --model-path Qwen/QwQ-32B --tp 2 --mem-fraction-static 0.9 --host 0.0.0.0 --port 40000
slurmstepd-nwonga100: error: *** JOB 11303 ON nwonga100 CANCELLED AT 2025-08-25T18:12:13 ***
