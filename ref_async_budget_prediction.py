import argparse
import httpx
import asyncio
import random
import numpy as np

import math
import time
import re
import os
import json
from tqdm.asyncio import tqdm
from tqdm import tqdm as sync_tqdm

from transformers import AutoTokenizer
from dataset import load_my_dataset
from agent import anyone_check  # ä¿®æ­£å¯¼å…¥

# --- [Original code for global variables and helper functions goes here, unchanged] ---

# Global variables for model clients and tokenizer
client_small = None
client_eval = None
semaphore = asyncio.Semaphore(4)  # å‡å°‘å¹¶å‘æ•°ï¼Œé™ä½æœåŠ¡å™¨å‹åŠ›

small_model_name = ""
eval_model_name = ""
tokenizer = None
small_tokenizer = None

# PPLæ•°ç»„æ•°æ®ï¼ˆç”¨äºç™¾åˆ†ä½æ•°è®¡ç®—ï¼‰
ppl_array = None
percentile_threshold = 0.5  # ç™¾åˆ†ä½æ•°é˜ˆå€¼ï¼Œé»˜è®¤50%åˆ†ä½æ•°


def build_question(question):
    if isinstance(question, str):
        return f"""
    Please answer the following problem using step-by-step reasoning.
    Please separate your reasoning steps with two newline characters (\\n\\n).
    Please must put your final answer within \\boxed{{}}.

    Question: {question}
    """
    elif isinstance(question, tuple):
        return f"""
    This is a multiple-choice question.
    Please answer the following problem using step-by-step reasoning.
    Separate each reasoning step with **two newline characters** (`\n\n`).
    You must put your final answer within \\boxed{{}}, such as \\boxed{{A}}, \\boxed{{B}}, \\boxed{{C}}, or \\boxed{{D}}. No other formats are allowed.

    Question: {question[0]}
    Choices:
    A. {question[1]}
    B. {question[2]}
    C. {question[3]}
    D. {question[4]}
    """
    return ""


def build_cot(history):
    return "\n\n".join([f"{h}" for h in history])


def build_small_init_prompt(question):
    return [
        {"role": "system", "content": "You are a math expert."},
        {"role": "user", "content": build_question(question)}
    ]


def build_small_inner_prompt(question, history):
    return [
        {"role": "user", "content": build_question(question)},
        {"role": "assistant", "content": build_cot(history)}
    ]


def build_eval_prompt_for_generate(question, history):
    return [
        {"role": "user", "content": build_question(question)},
        {"role": "assistant", "content": build_cot(history)}
    ]


def build_eval_prompt_for_eval(question, history):
    prompts = "\n\n".join([
        f"{history[i]}"
        for i in range(len(history))
    ])
    message = build_question(question) + "\n" + prompts
    return message


def load_ppl_array(ppl_array_path):
    """åŠ è½½PPLæ•°ç»„æ•°æ®æ–‡ä»¶"""
    global ppl_array
    if ppl_array_path and os.path.exists(ppl_array_path):
        ppl_array = np.load(ppl_array_path)
        print(f"âœ… æˆåŠŸåŠ è½½PPLæ•°ç»„æ•°æ®: {ppl_array.shape}")
        return True
    else:
        print(f"âŒ PPLæ•°ç»„æ–‡ä»¶ä¸å­˜åœ¨: {ppl_array_path}")
        return False


async def call_eval_model_ppl(prompt, idx, port):
    """
    Asynchronously calls the evaluation model to get the perplexity (PPL).
    """
    global client_eval, tokenizer
    message = build_eval_prompt_for_eval(prompt[0], prompt[1])
    last_history_item = prompt[1][-1].strip('\n')

    position = message.find(last_history_item)
    if position == -1:
        print(message)
        print("---------------------------")
        print(last_history_item)
        raise ValueError("Prompt tokens not found in full tokens.")

    sub_message = message[:position]
    logprob_start_len = len(tokenizer.tokenize(sub_message))

    payload = {
        "text": message,
        "sampling_params": {
            "temperature": 0,
            "max_new_tokens": 1,
        },
        "return_logprob": True,
        "logprob_start_len": logprob_start_len,
        "top_logprobs_num": 1,
    }

    global semaphore
    max_retries = 3
    for attempt in range(max_retries):
        try:
            async with semaphore:
                resp = await client_eval.post(
                    f"http://127.0.0.1:{port}/generate",
                    json=payload,
                    timeout=60.0  # å¢åŠ è¶…æ—¶æ—¶é—´
                )
                resp.raise_for_status()
                data = resp.json()
                input_token_logprobs = data['meta_info']['input_token_logprobs'][1:]
                logprobs = [entry[0] for entry in input_token_logprobs if entry[0] is not None]
                #print(f"ğŸ” PPL in here: {logprobs}")
                if not logprobs:
                    print(f"No log probabilities returned for problem: {prompt[0]}", flush=True)
                    return 0
                
                avg_neg_logprob = -sum(logprobs) / len(logprobs)
                
                return math.exp(avg_neg_logprob)
        except (httpx.ReadError, httpx.ConnectError, httpx.TimeoutException) as e:
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2  # æŒ‡æ•°é€€é¿
                print(f"âš ï¸  PPLè®¡ç®—å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}. ç­‰å¾… {wait_time}s åé‡è¯•...", flush=True)
                await asyncio.sleep(wait_time)
            else:
                print(f"âŒ PPLè®¡ç®—æœ€ç»ˆå¤±è´¥ (æ ·æœ¬ {idx}): {e}", flush=True)
                return 0  # è¿”å›é»˜è®¤å€¼
        except Exception as e:
            print(f"âŒ PPLè®¡ç®—å‡ºç°æœªçŸ¥é”™è¯¯ (æ ·æœ¬ {idx}): {e}", flush=True)
            return 0


def should_takeover_based_on_percentile(ppl_value):
    """åŸºäºPPLç™¾åˆ†ä½æ•°å†³å®šæ˜¯å¦æ¥ç®¡"""
    global ppl_array, percentile_threshold
    
    if ppl_array is None:
        return False
    rank = np.sum(ppl_array < ppl_value)
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    if ppl_array is not None:
        min_ppl = np.min(ppl_array)
        max_ppl = np.max(ppl_array)
        print(f"ğŸ” PPLå¯¹æ¯”: å½“å‰={ppl_value:.4f}, å†å²èŒƒå›´=[{min_ppl:.4f}, {max_ppl:.4f}], æ’å={rank}/{len(ppl_array)}")
    
    # è®¡ç®—å½“å‰PPLåœ¨å†å²åˆ†å¸ƒä¸­çš„ç™¾åˆ†ä½æ•°
    
    percentile = rank / len(ppl_array)
    
    should_takeover = percentile >= percentile_threshold
    
    if should_takeover:
        print(f"ğŸ¯ ç™¾åˆ†ä½æ•°è§¦å‘æ¥ç®¡: PPL={ppl_value:.4f}, ç™¾åˆ†ä½æ•°={percentile:.3f} >= {percentile_threshold}")
    
    return should_takeover


async def call_small_model(prompt, turn, max_tokens, idx, port):

    messages = (
        build_small_init_prompt(prompt[0]) if turn == 0 else build_small_inner_prompt(prompt[0], prompt[1])
    )
    
    global semaphore, client_small, small_model_name
    payload = {
        "model": small_model_name,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": max_tokens,
        "stop": ["\\boxed{"],  # æ·»åŠ åœæ­¢æ¡ä»¶ï¼Œä¸å†å²PPLè®¡ç®—ä¿æŒä¸€è‡´
    }
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            async with semaphore:
                resp = await client_small.post(
                    f"http://127.0.0.1:{port}/v1/chat/completions",
                    json=payload,
                    timeout=60.0
                )
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"]
        except (httpx.ReadError, httpx.ConnectError, httpx.TimeoutException) as e:
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"âš ï¸  å°æ¨¡å‹è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}. ç­‰å¾… {wait_time}s åé‡è¯•...", flush=True)
                await asyncio.sleep(wait_time)
            else:
                print(f"âŒ å°æ¨¡å‹è°ƒç”¨æœ€ç»ˆå¤±è´¥ (æ ·æœ¬ {idx}): {e}", flush=True)
                return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²
        except Exception as e:
            print(f"âŒ å°æ¨¡å‹è°ƒç”¨å‡ºç°æœªçŸ¥é”™è¯¯ (æ ·æœ¬ {idx}): {e}", flush=True)
            return ""


async def call_eval_model(prompt, max_tokens, idx, port):
    messages = build_eval_prompt_for_generate(prompt[0], prompt[1])
    global semaphore, client_eval, eval_model_name
    payload = {
        "model": eval_model_name,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": max_tokens,
        "stop": ["\\boxed{"],  # æ·»åŠ åœæ­¢æ¡ä»¶
    }

    max_retries = 3
    for attempt in range(max_retries):
        try:
            async with semaphore:
                resp = await client_eval.post(
                    f"http://127.0.0.1:{port}/v1/chat/completions",
                    json=payload,
                    timeout=60.0
                )
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"]
        except (httpx.ReadError, httpx.ConnectError, httpx.TimeoutException) as e:
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                print(f"âš ï¸  è¯„ä¼°æ¨¡å‹è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}. ç­‰å¾… {wait_time}s åé‡è¯•...", flush=True)
                await asyncio.sleep(wait_time)
            else:
                print(f"âŒ è¯„ä¼°æ¨¡å‹è°ƒç”¨æœ€ç»ˆå¤±è´¥ (æ ·æœ¬ {idx}): {e}", flush=True)
                return ""  # è¿”å›ç©ºå­—ç¬¦ä¸²
        except Exception as e:
            print(f"âŒ è¯„ä¼°æ¨¡å‹è°ƒç”¨å‡ºç°æœªçŸ¥é”™è¯¯ (æ ·æœ¬ {idx}): {e}", flush=True)
            return ""


async def extract_answer(history):
    answer = "invalid"
    temp = "\n\n".join([
        f"{history[i]}"
        for i in range(len(history))
    ])

    # å°è¯•å¤šç§ç­”æ¡ˆæ ¼å¼
    # 1. \boxed{} æ ¼å¼
    matches = re.findall(r"\\boxed\{(.*?)\}", temp)
    if matches:
        answer = matches[-1].strip()
        return answer
    
    # 2. ANSWER: æ ¼å¼
    pattern = re.compile(r"ANSWER:\s*([A-Z])", re.IGNORECASE)
    matches = pattern.findall(temp)
    if matches:
        answer = matches[-1].strip()
        return answer
    
    # 3. æŸ¥æ‰¾å¸¸è§çš„ç­”æ¡ˆæ¨¡å¼
    patterns = [
        r"answer[:\s]*([A-Z])",  # answer: A
        r"the answer is[:\s]*([A-Z])",  # the answer is A
        r"final answer[:\s]*([A-Z])",  # final answer A
        r"option[:\s]*([A-Z])",  # option A
        r"choice[:\s]*([A-Z])",  # choice A
    ]
    
    for pattern_str in patterns:
        pattern = re.compile(pattern_str, re.IGNORECASE)
        matches = pattern.findall(temp)
        if matches:
            answer = matches[-1].strip()
            return answer
    
    # 4. å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›æœ€åä¸€ä¸ªéç©ºè¡Œçš„å†…å®¹ï¼ˆä½œä¸ºè°ƒè¯•ä¿¡æ¯ï¼‰
    lines = temp.strip().split('\n')
    for line in reversed(lines):
        line = line.strip()
        if line and len(line) <= 10:  # å‡è®¾ç­”æ¡ˆä¸ä¼šå¤ªé•¿
            return line
    
    return answer


async def process_single_problem(problem, small_model_max_tokens, evalator_max_tokens, turns, idx, small_model_port, eval_model_port, output_dir, repeats=1, takeover_stats=None, takeover_budget=None):
    prompt = [problem, []]
    answer = "invalid"
    start_time = time.time()
    
    # è®¡ç®—é—®é¢˜ç»„ç´¢å¼•
    problem_group_idx = idx // repeats
    
    history_log = []
    problem_has_takeover = False
    temp = None
    for turn in range(turns):
        print(f"ğŸ“Š Problem Group {problem_group_idx} (Sample {idx}) - Turn {turn+1}/{turns}", flush=True)
        small_out = await call_small_model(prompt, turn, small_model_max_tokens, idx, small_model_port)
        print(f"ğŸ”¹ å°æ¨¡å‹è¾“å‡º (Turn {turn+1}): {small_out[:200]}{'...' if len(small_out) > 200 else ''}")
        history_log.append({"turn": turn, "model": "small", "output": small_out})
        prompt[1].append(small_out)

        if not small_out:
            print("Small model returned empty output.", flush=True)
            break

        # å®æ—¶è®¡ç®—PPLå¹¶åŸºäºç™¾åˆ†ä½æ•°å†³å®šæ˜¯å¦æ¥ç®¡
        ppl = await call_eval_model_ppl(prompt, idx, eval_model_port)
        should_takeover = should_takeover_based_on_percentile(ppl)
        
        history_log.append({"turn": turn, "model": "eval_ppl", "ppl": ppl, "should_takeover": int(should_takeover)})
        
        if should_takeover:
            print(f"ğŸ¯ Turn {turn+1}: ç™¾åˆ†ä½æ•°è§¦å‘æ¥ç®¡! (Sample {idx}, PPL={ppl:.4f})", flush=True)
            eval_out = await call_eval_model(prompt, evalator_max_tokens, idx, eval_model_port)
            print(f"ğŸ”¸ å¤§æ¨¡å‹è¾“å‡º (Turn {turn+1}): {eval_out[:200]}{'...' if len(eval_out) > 200 else ''}")
            history_log.append({"turn": turn, "model": "eval_generate", "output": eval_out})
            prompt[1].append(eval_out)
            problem_has_takeover = True
            if takeover_stats:
                takeover_stats['total_takeovers'] += 1
                # è®°å½•å½“å‰æ ·æœ¬çš„æ¥ç®¡æ¬¡æ•°
                if idx not in takeover_stats['sample_takeovers']:
                    takeover_stats['sample_takeovers'][idx] = 0
                takeover_stats['sample_takeovers'][idx] += 1
        else:
            print(f"â­ï¸  Turn {turn+1}: è·³è¿‡æ¥ç®¡ (Sample {idx}, PPL={ppl:.4f})", flush=True)
        
        # è®¡ç®—å¹¶æ‰“å°æ¥ç®¡ç‡æ¯”è¾ƒï¼ˆæ–¹æ³•äºŒï¼‰- ä»…åœ¨groupå†…æœ€åä¸€ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªturnæ‰“å°
        if takeover_stats and takeover_budget is not None:
            # æ£€æŸ¥æ˜¯å¦æ˜¯groupå†…æœ€åä¸€ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªturn
            is_last_sample_in_group = (idx % repeats == repeats - 1)  # æœ€åä¸€ä¸ªæ ·æœ¬
            is_last_turn = (turn == turns - 1)  # æœ€åä¸€ä¸ªturn
            is_early_stop = (temp != "invalid")  # æå‰åœæ­¢
            
            if (is_last_sample_in_group and (is_last_turn or is_early_stop)):
                # æ–¹æ³•äºŒï¼šåŸºäºæ ·æœ¬æ•°é‡è®¡ç®—
                theoretical_takeovers = takeover_budget  # ç†è®ºæ¥ç®¡æ¬¡æ•°
                # è®¡ç®—å½“å‰groupçš„å®é™…æ¥ç®¡æ¬¡æ•°
                current_group_takeovers = 0
                start_sample_idx = problem_group_idx * repeats
                end_sample_idx = start_sample_idx + repeats
                
                # ä»takeover_statsä¸­ç»Ÿè®¡å½“å‰groupçš„æ¥ç®¡æ¬¡æ•°
                for sample_idx in range(start_sample_idx, end_sample_idx):
                    if sample_idx in takeover_stats.get('sample_takeovers', {}):
                        current_group_takeovers += takeover_stats['sample_takeovers'][sample_idx]
                
                accuracy = 1 - abs(current_group_takeovers - theoretical_takeovers) / theoretical_takeovers
                print(f"ğŸ“ˆ Group {problem_group_idx} å®Œæˆ - æ¥ç®¡ç‡æ¯”è¾ƒ: ç†è®º={theoretical_takeovers}æ¬¡, å®é™…={current_group_takeovers}æ¬¡, å‡†ç¡®ç‡={accuracy:.1%}", flush=True)

        temp = await extract_answer(prompt[1])
        if temp != "invalid":
            answer = temp
            print("Early stop due to valid answer found.")
            break

    if answer == "invalid":
        answer = await extract_answer(prompt[1])

    end_time = time.time()
    duration = end_time - start_time
    
    result_data = {
        "problem_index": idx,
        "final_answer": answer,
        "duration_seconds": duration,
        "full_history": history_log,
        "question": problem
    }
    
    output_filename = os.path.join(output_dir, f"problem_{idx:04d}.json")
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, indent=4)
    
    # æ›´æ–°æ¥ç®¡ç»Ÿè®¡
    if takeover_stats:
        if problem_has_takeover:
            takeover_stats['problems_with_takeover'].add(problem_group_idx)
        else:
            takeover_stats['problems_without_takeover'].add(problem_group_idx)
            # è®°å½•æ²¡æœ‰æ¥ç®¡çš„é—®é¢˜è¯¦æƒ…
            takeover_stats['no_takeover_details'].append({
                'problem_group_idx': problem_group_idx,
                'sample_idx': idx,
                'ppl_values': [entry.get('ppl', 0) for entry in history_log if entry.get('model') == 'eval_ppl'],
                'question': problem[:200] + '...' if len(problem) > 200 else problem
            })
        
    # We don't need to return anything, as the result is already saved.
    return ()


async def compute_score(results, answers, repeats, takeover_stats=None, takeover_budget=None):
    generated_ans = [ans for ans, _ in results]
    group = len(generated_ans) // repeats
    right = 0
    
    print(f"\nğŸ“Š ç­”æ¡ˆç»Ÿè®¡:")
    print(f"æ€»é—®é¢˜ç»„æ•°: {group}")
    print(f"é‡å¤æ¬¡æ•°: {repeats}")
    
    for i in range(group):
        start = i * repeats
        end = (i + 1) * repeats
        outputs = generated_ans[start:end]
        correct_answer = answers[start]
        
        # ç®€å•çš„ç­”æ¡ˆåŒ¹é…ï¼ˆé¿å…ä½¿ç”¨å¤–éƒ¨APIï¼‰
        matched = False
        for output in outputs:
            if output != "invalid" and output == correct_answer:
                matched = True
                break
        
        print(f"é—®é¢˜ç»„ {i}: æ­£ç¡®ç­”æ¡ˆ={correct_answer}, ç”Ÿæˆç­”æ¡ˆ={outputs}, åŒ¹é…={matched}")
        
        if matched:
            right += 1

    accuracy = right / group if group > 0 else 0
    print(f"\nğŸ¯ æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.2%} ({right}/{group})")
    
    # ç»Ÿè®¡æ— æ•ˆç­”æ¡ˆ
    invalid_count = sum(1 for ans in generated_ans if ans == "invalid")
    print(f"âš ï¸  æ— æ•ˆç­”æ¡ˆæ•°é‡: {invalid_count}/{len(generated_ans)} ({invalid_count/len(generated_ans)*100:.1f}%)")
    
    # æ˜¾ç¤ºæœ€ç»ˆçš„æ€»æ¥ç®¡æ•°é‡
    if takeover_stats:
        total_takeovers = takeover_stats['total_takeovers']
        print(f"ğŸ“Š æœ€ç»ˆæ€»æ¥ç®¡æ•°é‡: {total_takeovers}")


async def main():
    parser = argparse.ArgumentParser(description="Run a multi-turn, multi-agent evaluation.")
    parser.add_argument("--small_model_name", type=str, required=True,
                        help="Name of the small model for generating responses.")
    parser.add_argument("--eval_model_name", type=str, required=True,
                        help="Name of the model to use for PPL evaluation.")
    parser.add_argument("--dataset_name", type=str, required=True,
                        help="Name of the dataset to use (e.g., gpqa, math500).")

    parser.add_argument("--turns", type=int, default=15,
                        help="Maximum number of turns for the multi-agent loop.")
    parser.add_argument("--small_model_max_tokens", type=int, default=500,
                        help="Maximum tokens for the small model's response.")
    parser.add_argument("--evalator_max_tokens", type=int, default=500,
                        help="Maximum tokens for the evaluation model's response.")
    parser.add_argument("--repeats", type=int, default=16,
                        help="Number of times to repeat each problem.")
    parser.add_argument("--small_model_port", type=int, default=51101,
                        help="Port for the small model server.")
    parser.add_argument("--eval_model_port", type=int, default=51100,
                        help="Port for the evaluation model server.")
    parser.add_argument("--output_dir", type=str, required=True,
                        help="Directory to save the results and history.")

    parser.add_argument("--takeover_budget", type=int, default=10,
                        help="Global budget for evaluation model takeovers (default: 10)")
    parser.add_argument("--ppl_array_path", type=str, default=None,
                        help="Path to PPL array file (.npy) for percentile calculation")
    parser.add_argument("--percentile_threshold", type=float, default=0.5,
                        help="Percentile threshold for triggering takeover (default: 0.5, 50th percentile)")
    
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    
    global client_small, client_eval, small_model_name, eval_model_name, tokenizer, small_tokenizer, percentile_threshold
    small_model_name = args.small_model_name
    eval_model_name = args.eval_model_name
    percentile_threshold = args.percentile_threshold
    
    # åŠ è½½PPLæ•°ç»„æ•°æ®
    if args.ppl_array_path:
        if not load_ppl_array(args.ppl_array_path):
            print("âš ï¸  æ— æ³•åŠ è½½PPLæ•°ç»„æ–‡ä»¶ï¼Œå°†ä½¿ç”¨éšæœºæ¥ç®¡ç­–ç•¥")
    else:
        print("âš ï¸  æœªæä¾›PPLæ•°ç»„æ–‡ä»¶è·¯å¾„ï¼Œå°†ä½¿ç”¨éšæœºæ¥ç®¡ç­–ç•¥")

    client_small = httpx.AsyncClient(
        timeout=240.0,
        limits=httpx.Limits(max_connections=1000, max_keepalive_connections=1000)
    )
    client_eval = httpx.AsyncClient(
        timeout=240.0,
        limits=httpx.Limits(max_connections=1000, max_keepalive_connections=1000)
    )

    tokenizer = AutoTokenizer.from_pretrained(args.eval_model_name)
    tokenizer.use_default_system_prompt = True
    small_tokenizer = AutoTokenizer.from_pretrained(args.small_model_name)
    small_tokenizer.use_default_system_prompt = True

    context, answer = load_my_dataset(args.dataset_name, args.repeats)
    
    total_unique_problems = len(answer) // args.repeats
    total_samples = len(context)
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: {total_unique_problems}ä¸ªé—®é¢˜, {total_samples}ä¸ªæ ·æœ¬")
    if ppl_array is not None:
        print(f"ğŸ“Š PPLæ•°ç»„ä¿¡æ¯: {ppl_array.shape}, ç™¾åˆ†ä½æ•°é˜ˆå€¼: {percentile_threshold}")
        # ç»Ÿè®¡ç™¾åˆ†ä½æ•°åˆ†å¸ƒ
        expected_takeover_count = int(len(ppl_array) * percentile_threshold)
        print(f"ğŸ“Š é¢„æœŸæ¥ç®¡æ ·æœ¬æ•°: {expected_takeover_count}/{len(ppl_array)} ({percentile_threshold*100:.1f}%åˆ†ä½æ•°)")
    
    # æ·»åŠ æ¥ç®¡ç»Ÿè®¡å˜é‡
    takeover_stats = {
        'total_takeovers': 0,
        'problems_with_takeover': set(),
        'problems_without_takeover': set(),
        'no_takeover_details': [],
        'sample_takeovers': {}  # æ–°å¢ï¼šæ¯ä¸ªæ ·æœ¬çš„æ¥ç®¡æ¬¡æ•°
    }
    

    
    # æœ€ç»ˆä¿®æ­£çš„ã€æ­£ç¡®çš„æ–­ç‚¹æ¢å¤å’Œåˆ†ç»„å¤„ç†é€»è¾‘
    
    # ç¬¬1æ­¥ï¼šæ‰¾å‡ºæ‰€æœ‰å·²å®Œæˆçš„å•ä¸ªé‡‡æ ·ä»»åŠ¡çš„ç´¢å¼•
    processed_sample_indices = set()
    for filename in os.listdir(args.output_dir):
        if filename.startswith("problem_") and filename.endswith(".json"):
            try:
                sample_idx = int(filename.replace("problem_", "").replace(".json", ""))
                processed_sample_indices.add(sample_idx)
            except ValueError:
                continue

    # ç¬¬2æ­¥ï¼šè¯†åˆ«æ‰€æœ‰éœ€è¦å¤„ç†çš„å”¯ä¸€é—®é¢˜ç»„
    unique_problems_to_process = []
    for unique_idx in range(total_unique_problems):
        start_idx = unique_idx * args.repeats
        end_idx = start_idx + args.repeats
        
        is_group_incomplete = any(
            (idx not in processed_sample_indices) for idx in range(start_idx, end_idx)
        )
        if is_group_incomplete:
            unique_problems_to_process.append(unique_idx)
    
    if not unique_problems_to_process:
        print("æ‰€æœ‰é—®é¢˜éƒ½å·²å®Œæˆå¤„ç†ã€‚æ— éœ€è¿è¡Œæ–°ä»»åŠ¡ã€‚")
        all_results = []
        for idx in range(total_samples):
            filepath = os.path.join(args.output_dir, f"problem_{idx:04d}.json")
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_results.append((data['final_answer'], data['duration_seconds']))
        await compute_score(all_results, answer, args.repeats, takeover_stats, args.takeover_budget)
        return

    print(f"æ‰¾åˆ° {len(unique_problems_to_process)} ä¸ªéœ€è¦å¤„ç†çš„é—®é¢˜ç»„ã€‚æ­£åœ¨æ¢å¤...")
    
    start_time = time.time()
    
    # ç¬¬3æ­¥ï¼šæŒ‰"é—®é¢˜ç»„"ä¸ºå•ä½ï¼Œåªå¤„ç†ç»„å†…æœªå®Œæˆçš„é‡‡æ ·ä»»åŠ¡
    for unique_idx in sync_tqdm(unique_problems_to_process, desc="Processing problem groups"):
        print(f"ğŸ”„ Processing Problem Group {unique_idx}")
        
        # è®¡ç®—å½“å‰é—®é¢˜ç»„çš„æ ·æœ¬èŒƒå›´
        start_sample_idx = unique_idx * args.repeats
        end_sample_idx = start_sample_idx + args.repeats
        
        tasks_to_run_for_group = []
        
        for sample_idx in range(start_sample_idx, end_sample_idx):
            # æ£€æŸ¥è¿™ä¸ªé‡‡æ ·æ˜¯å¦å·²ç»å®Œæˆ
            if sample_idx not in processed_sample_indices:
                problem = context[sample_idx]
                task = asyncio.create_task(
                    process_single_problem(
                        problem,
                        args.small_model_max_tokens,
                        args.evalator_max_tokens,
                        args.turns,
                        sample_idx,
                        args.small_model_port,
                        args.eval_model_port,
                        args.output_dir,
                        args.repeats,
                        takeover_stats,
                        args.takeover_budget
                    )
                )
                tasks_to_run_for_group.append(task)
        
        # åœ¨è¿™é‡Œæ‰§è¡Œæœ¬ç»„å†…çš„æ‰€æœ‰ä»»åŠ¡ï¼Œå¹¶ç­‰å¾…å®ƒä»¬å…¨éƒ¨å®Œæˆ
        if tasks_to_run_for_group:
            await tqdm.gather(*tasks_to_run_for_group, desc=f"Group {unique_idx} samples")
            # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ”¶é›†è¿”å›å€¼ï¼Œå› ä¸ºä¿å­˜æ“ä½œåœ¨ä»»åŠ¡å†…éƒ¨å·²ç»å®Œæˆ
            
            # æ¯ä¸ªé—®é¢˜ç»„å®Œæˆåè®¡ç®—æ¥ç®¡ç‡å‡†ç¡®ç‡
            if takeover_stats and args.takeover_budget is not None:
                # è®¡ç®—å½“å‰é—®é¢˜ç»„çš„æ¥ç®¡æ¬¡æ•°
                current_group_takeovers = sum(
                    takeover_stats.get('sample_takeovers', {}).get(sample_idx, 0)
                    for sample_idx in range(start_sample_idx, end_sample_idx)
                )
                
                theoretical_total_takeovers = args.takeover_budget
                takeover_accuracy = 1 - abs(current_group_takeovers - theoretical_total_takeovers) / theoretical_total_takeovers
                
                print(f"\nğŸ“ˆ é—®é¢˜ç»„ {unique_idx} å®Œæˆåçš„æ¥ç®¡ç‡å‡†ç¡®ç‡ç»Ÿè®¡:")
                print(f"ç†è®ºæ€»æ¥ç®¡æ¬¡æ•°: {theoretical_total_takeovers} (é¢„ç®—: {args.takeover_budget})")
                print(f"å½“å‰é—®é¢˜ç»„å®é™…æ¥ç®¡æ¬¡æ•°: {current_group_takeovers}")
                print(f"æ¥ç®¡ç‡å‡†ç¡®ç‡: {takeover_accuracy:.1%}")
                print(f"å½“å‰é—®é¢˜ç»„æ ·æœ¬æ•°: {args.repeats}")
                print("-" * 50)
            
    end_time = time.time()
    print(f"è€—æ—¶: {end_time - start_time:.3f} s")
    
    # æ‰“å°æ¥ç®¡ç»Ÿè®¡å’Œæ²¡æœ‰è¢«æ¥ç®¡çš„éƒ¨åˆ†
    print("\n" + "="*60)
    print("ğŸ“Š æ¥ç®¡æƒ…å†µç»Ÿè®¡")
    print("="*60)
    
    total_problems_processed = len(takeover_stats['problems_with_takeover']) + len(takeover_stats['problems_without_takeover'])
    print(f"æ€»å¤„ç†é—®é¢˜ç»„æ•°: {total_problems_processed}")
    print(f"æœ‰æ¥ç®¡çš„é—®é¢˜ç»„: {len(takeover_stats['problems_with_takeover'])}")
    print(f"æ— æ¥ç®¡çš„é—®é¢˜ç»„: {len(takeover_stats['problems_without_takeover'])}")
    print(f"æ€»æ¥ç®¡æ¬¡æ•°: {takeover_stats['total_takeovers']}")
    
    if takeover_stats['problems_without_takeover']:
        print(f"\nâŒ æ²¡æœ‰è¢«æ¥ç®¡çš„é—®é¢˜ç»„: {sorted(takeover_stats['problems_without_takeover'])}")
        
        print("\n" + "="*60)
        print("ğŸ” æ²¡æœ‰è¢«æ¥ç®¡çš„è¯¦ç»†æƒ…å†µ")
        print("="*60)
        
        for detail in takeover_stats['no_takeover_details']:
            print(f"\né—®é¢˜ç»„ {detail['problem_group_idx']} (æ ·æœ¬ {detail['sample_idx']}):")
            print(f"é—®é¢˜: {detail['question']}")
            print(f"PPLå€¼: {[f'{p:.4f}' for p in detail['ppl_values']]}")
            print(f"å¹³å‡PPL: {sum(detail['ppl_values'])/len(detail['ppl_values']):.4f}")
            print("-" * 40)
    
    # è®¡ç®—æ¥ç®¡ç‡
    if total_problems_processed > 0:
        takeover_rate = len(takeover_stats['problems_with_takeover']) / total_problems_processed * 100
        print(f"\nğŸ“ˆ é—®é¢˜ç»„æ¥ç®¡ç‡: {takeover_rate:.1f}%")
    
    print("="*60)
    
    # æœ€åï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆä¹‹åï¼Œæˆ‘ä»¬æ‰å»è®¡ç®—æœ€ç»ˆåˆ†æ•°
    print("\nå°è¯•è®¡ç®—æœ€ç»ˆåˆ†æ•°...")
    
    all_files_exist = True
    for idx in range(total_samples):
        filepath = os.path.join(args.output_dir, f"problem_{idx:04d}.json")
        if not os.path.exists(filepath):
            print(f"é”™è¯¯ï¼šæ‰€éœ€ç»“æœæ–‡ä»¶ {filepath} ç¼ºå¤±ã€‚æ— æ³•è®¡ç®—æœ€ç»ˆåˆ†æ•°ã€‚")
            all_files_exist = False
            break
            
    if all_files_exist:
        all_results = []
        for idx in range(total_samples):
            filepath = os.path.join(args.output_dir, f"problem_{idx:04d}.json")
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_results.append((data['final_answer'], data['duration_seconds']))
        await compute_score(all_results, answer, args.repeats, takeover_stats, args.takeover_budget)
    else:
        print("ç”±äºç»“æœæ–‡ä»¶ç¼ºå¤±ï¼Œå°†ä¸è®¡ç®—æœ€ç»ˆåˆ†æ•°ã€‚è¯·é‡æ–°è¿è¡Œè„šæœ¬ä»¥å®Œæˆæ‰€æœ‰ä»»åŠ¡ã€‚")

    await client_small.aclose()
    await client_eval.aclose()

if __name__ == "__main__":
    asyncio.run(main())