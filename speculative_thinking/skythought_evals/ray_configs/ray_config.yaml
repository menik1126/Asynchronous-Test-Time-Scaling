llm_engine: vllm # currently only vllm supported
accelerator_type: A100  # 改为A100，因为日志显示使用的是A100 GPU
engine_kwargs: # vllm engine kwargs 
  tensor_parallel_size: 1  # 从4改为1，减少GPU需求
  gpu_memory_utilization: 0.9
  # other optional vllm engine kwargs to tune performance!
  # pipeline_parallel_size: 1
  # max_num_seqs: 448
  # use_v2_block_manager: True
  # enable_prefix_caching: False
  # preemption_mode: "recompute"
  # block_size: 16
  # kv_cache_dtype: "auto"
  # enforce_eager: False
  # enable_chunked_prefill: True
  max_num_batched_tokens: 8192
  max_seq_len_to_capture: 8192
runtime_env:
  env_vars:
    VLLM_ATTENTION_BACKEND: "FLASH_ATTN"
env_config:
  num_replicas: 1  # 从2改为1，减少副本数量
  batch_size: 64  # 从128改为64，减少批处理大小
